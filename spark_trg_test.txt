import org.apache.hadoop.fs.Path
import java.time.{ZonedDateTime, ZoneId}
import java.time.format.DateTimeFormatter

val sparconf = new sparkconf().setmaster("local[*]").setAppName("crimeRate")
val sc = new Sparkcontext(sparkconf)



val inputPath = "hadoop/user/src/2019-01"

val startDate = "2019-01"
val endDate = "2020-03"

val baseFolder = new Path(inputPath.substring(0, inputPath.lastIndexOf("/") + 1))

val files = baseFolder.getFileSystem(sc.hadoopConfiguration).listStatus(baseFolder).map(_.getPath.toString)
val filtredFiles = files.filter(path => path.split("/").last > startDate &&  path.split("/").last < endDate)

// finally load only the folders you want
val df_Outcomes= spark.read.csv(filtredFiles+"-outcomes": _*) 

val df_street =spark.read.csv(filtredFiles+"-street": _*)

val distinctName = files.substring(9,files.lastIndexOf("-") +1)

val newDf = df_Outcomes.join(df_street,df_street("Crime ID") ==  df_Outcomes("Crime ID"), "inner").withCOlumn("Crime ID",col("Crime ID")).withCOlumn("DistinctName",lit(distinctName)).withCOlumn("latitude",col("latitude")).withCOlumn("longitude",col("longitude")).withCOlumn("Crime type",col("Crime type")).with("Outcome type",col("Outcome type"))

newdDf.write.mode("append").parquet("/tmp/output/crime.parquet")


newdDf.write.mode(SaveMode.Overwrite)
  .json("hdfs:///tmp/json/zipcodes.json")
  

def get(url: String): String = scala.io.Source.fromURL("hdfs://quickstart.cloudera:8020/user/tmp/json/").mkString
  



import org.apache.spark.sql.types._                        
import org.apache.spark.sql.functions._                    
val jsonSchema = new StructType()
.add("Crime ID", LongType)
.add("DistinctName", StringType)
.add("longitude",StringType)
.add("latitude", StringType)
.add("Crime type", StringType)
.add("device_type", StringType)
.add("Outcome type", StringType)
        
		val df = spark.createDataFrame(newdDf.rdd, jsonSchema)4
		
		df.printschema()
		

